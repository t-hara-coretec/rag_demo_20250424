{"content": "Debugging and Monitoring - PydanticAI\nSkip to content\nDebugging and Monitoring\nApplications that use LLMs have some challenges that are well known and understood: LLMs are\nslow\n,\nunreliable\nand\nexpensive\n.\nThese applications also have some challenges that most developers have encountered much less often: LLMs are\nfickle\nand\nnon-deterministic\n. Subtle changes in a prompt can completely change a model's performance, and there's no\nEXPLAIN\nquery you can run to understand why.\nWarning\nFrom a software engineers point of view, you can think of LLMs as the worst database you've ever heard of, but worse.\nIf LLMs weren't so bloody useful, we'd never touch them.\nTo build successful applications with LLMs, we need new tools to understand both model performance, and the behavior of applications that rely on them.\nLLM Observability tools that just let you understand how your model is performing are useless: making API calls to an LLM is easy, it's building that into an application that's hard.\nPydantic Logfire\nPydantic Logfire\nis an observability platform developed by the team who created and maintain Pydantic and PydanticAI. Logfire aims to let you understand your entire application: Gen AI, classic predictive AI, HTTP traffic, database queries and everything else a modern application needs.\nPydantic Logfire is a commercial product\nLogfire is a commercially supported, hosted platform with an extremely generous and perpetual\nfree tier\n.\nYou can sign up and start using Logfire in a couple of minutes.\nPydanticAI has built-in (but optional) support for Logfire. That means if the\nlogfire\npackage is installed and configured and agent instrumentation is enabled then detailed information about agent runs is sent to Logfire. Otherwise there's virtually no overhead and nothing is sent.\nHere's an example showing details of running the\nWeather Agent\nin Logfire:\nUsing Logfire\nTo use logfire, you'll need a logfire\naccount\n, and logfire installed:\npip\nuv\npip\ninstall\n\"pydantic-ai[logfire]\"\nuv\nadd\n\"pydantic-ai[logfire]\"\nThen authenticate your local environment with logfire:\npip\nuv\nlogfire\nauth\nuv\nrun\nlogfire\nauth\nAnd configure a project to send data to:\npip\nuv\nlogfire\nprojects\nnew\nuv\nrun\nlogfire\nprojects\nnew\n(Or use an existing project with\nlogfire projects use\n)\nThen add logfire to your code:\nadding_logfire.py\nimport\nlogfire\nlogfire\n.\nconfigure\n()\nand enable instrumentation in your agent:\ninstrument_agent.py\nfrom\npydantic_ai\nimport\nAgent\nagent\n=\nAgent\n(\n'openai:gpt-4o'\n,\ninstrument\n=\nTrue\n)\n# or instrument all agents to avoid needing to add `instrument=True` to each agent:\nAgent\n.\ninstrument_all\n()\nThe\nlogfire documentation\nhas more details on how to use logfire,\nincluding how to instrument other libraries like\nPydantic\n,\nHTTPX\nand\nFastAPI\n.\nSince Logfire is built on\nOpenTelemetry\n, you can use the Logfire Python SDK to send data to any OpenTelemetry collector.\nOnce you have logfire set up, there are two primary ways it can help you understand your application:\nDebugging\nâ Using the live view to see what's happening in your application in real-time.\nMonitoring\nâ Using SQL and dashboards to observe the behavior of your application, Logfire is effectively a SQL database that stores information about how your application is running.\nDebugging\nTo demonstrate how Logfire can let you visualise the flow of a PydanticAI run, here's the view you get from Logfire while running the\nchat app examples\n:\nMonitoring Performance\nWe can also query data with SQL in Logfire to monitor the performance of an application. Here's a real world example of using Logfire to monitor PydanticAI runs inside Logfire itself:\nMonitoring HTTPX Requests\nIn order to monitor HTTPX requests made by models, you can use\nlogfire\n's\nHTTPX\nintegration.\nInstrumentation is as easy as adding the following three lines to your application:\ninstrument_httpx.py\nimport\nlogfire\nlogfire\n.\nconfigure\n()\nlogfire\n.\ninstrument_httpx\n(\ncapture_all\n=\nTrue\n)\n# (1)!\nSee the\nlogfire docs\nfor more\nhttpx\ninstrumentation details.\nIn particular, this can help you to trace specific requests, responses, and headers:\ninstrument_httpx_example.py\nimport\nlogfire\nfrom\npydantic_ai\nimport\nAgent\nlogfire\n.\nconfigure\n()\nlogfire\n.\ninstrument_httpx\n(\ncapture_all\n=\nTrue\n)\n# (1)!\nagent\n=\nAgent\n(\n'openai:gpt-4o'\n,\ninstrument\n=\nTrue\n)\nresult\n=\nagent\n.\nrun_sync\n(\n'What is the capital of France?'\n)\nprint\n(\nresult\n.\ndata\n)\n# > The capital of France is Paris.\nCapture all of headers, request body, and response body.\nWith\nhttpx\ninstrumentation\nWithout\nhttpx\ninstrumentation\nTip\nhttpx\ninstrumentation might be of particular utility if you're using a custom\nhttpx\nclient in your model in order to get insights into your custom requests.\nUsing OpenTelemetry\nPydanticAI's instrumentation uses\nOpenTelemetry\n, which Logfire is based on. You can use the Logfire SDK completely freely and follow the\nAlternative backends\nguide to send the data to any OpenTelemetry collector, such as a self-hosted Jaeger instance. Or you can skip Logfire entirely and use the OpenTelemetry Python SDK directly.\nData format\nPydanticAI follows the\nOpenTelemetry Semantic Conventions for Generative AI systems\n, with one caveat. The semantic conventions specify that messages should be captured as individual events (logs) that are children of the request span. By default, PydanticAI instead collects these events into a JSON array which is set as a single large attribute called\nevents\non the request span. To change this, use\nInstrumentationSettings(event_mode='logs')\n.\ninstrumentation_settings_event_mode.py\nfrom\npydantic_ai\nimport\nAgent\nfrom\npydantic_ai.agent\nimport\nInstrumentationSettings\ninstrumentation_settings\n=\nInstrumentationSettings\n(\nevent_mode\n=\n'logs'\n)\nagent\n=\nAgent\n(\n'openai:gpt-4o'\n,\ninstrument\n=\ninstrumentation_settings\n)\n# or instrument all agents:\nAgent\n.\ninstrument_all\n(\ninstrumentation_settings\n)\nFor now, this won't look as good in the Logfire UI, but we're working on it.\nIf you have very long conversations, the\nevents\nspan attribute may be truncated. Using\nevent_mode='logs'\nwill help avoid this issue.\nNote that the OpenTelemetry Semantic Conventions are still experimental and are likely to change.\nSetting OpenTelemetry SDK providers\nBy default, the global\nTracerProvider\nand\nEventLoggerProvider\nare used. These are set automatically by\nlogfire.configure()\n. They can also be set by the\nset_tracer_provider\nand\nset_event_logger_provider\nfunctions in the OpenTelemetry Python SDK. You can set custom providers with\nInstrumentationSettings\n.\ninstrumentation_settings_providers.py\nfrom\nopentelemetry.sdk._events\nimport\nEventLoggerProvider\nfrom\nopentelemetry.sdk.trace\nimport\nTracerProvider\nfrom\npydantic_ai.agent\nimport\nInstrumentationSettings\ninstrumentation_settings\n=\nInstrumentationSettings\n(\ntracer_provider\n=\nTracerProvider\n(),\nevent_logger_provider\n=\nEventLoggerProvider\n(),\n)\nInstrumenting a specific\nModel\ninstrumented_model_example.py\nfrom\npydantic_ai\nimport\nAgent\nfrom\npydantic_ai.models.instrumented\nimport\nInstrumentationSettings\n,\nInstrumentedModel\nsettings\n=\nInstrumentationSettings\n()\nmodel\n=\nInstrumentedModel\n(\n'gpt-4o'\n,\nsettings\n)\nagent\n=\nAgent\n(\nmodel\n)", "metadata": {"source": "https://ai.pydantic.dev/logfire/#data-format", "title": "Debugging and Monitoring - PydanticAI", "domain": "ai.pydantic.dev", "type": "web"}}